{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbb4b0b",
   "metadata": {},
   "source": [
    "# # CP 217: Machine Learning for Cyber-Physical Systems \n",
    "## Week 3: Linear Regression, Polynomial Regression, Regularization\n",
    "***\n",
    "In this workshop, first we'll look at linear regression method. Briefly, this involves learning a linear regression model from a training set of $(\\mathbf{x}, y)$ pairs, where $\\mathbf{x}$ is a feature vector and $y$ is a real-valued response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import pandas as pd\n",
    "#import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353e743",
   "metadata": {},
   "source": [
    "### 1. Review\n",
    "In this week's lecture, we saw that a linear model can be expressed as:\n",
    "$$y = w_0 + \\sum_{j = 1}^{m} w_j x_j = \\mathbf{w} \\cdot \\mathbf{x} $$\n",
    "where \n",
    "\n",
    "* $y$ is the *target (or output) variable*;\n",
    "* $\\mathbf{x} = [x_1, \\ldots, x_m]$ is a vector of *features* or *predictors* (we define $x_0 = 1$); and\n",
    "* $\\mathbf{w} = [w_0, \\ldots, w_m]$ are the *weights*.\n",
    "\n",
    "To fit the model, we will *minimise* the residual sum of squares (RSS) (simple case) which is also an MLE estimate for linear regression:\n",
    "\n",
    "$$RSS(\\mathbf{w}) = \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\mathbf{x}_i)^2$$\n",
    "\n",
    "**Note:** For simplicity, we'll consider the case $m = 1$ (i.e. only one feature excluding the intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8507b",
   "metadata": {},
   "source": [
    "***Problem Statement***\n",
    "\n",
    "You are given the air quality data of few major cities in India. This data includes several air quality related variables such as PM2.5, PM10, NOx, CO etc. and Air quality index (AQI). Your task is to learn a regression model on the given data to predict AQI from the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf533158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load air quality index data  (https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india)\n",
    "data=pd.read_csv('AQI_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c10c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing probable outliers (\n",
    "index_names = data[ data['AQI'] > 600 ].index  # you can use z-score to find the indices of outliers in AQI)\n",
    "data.drop(index_names, inplace = True) #drop index_names from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_=data.iloc[:,1]  #choosing second col of data \n",
    "y_=data.iloc[:,11] #choosing AQI col of data which will act as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic preprocessing\n",
    "x_=np.array(x_)\n",
    "y_=np.array(y_)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for i in range(len(data)):\n",
    "    x.append([x_[i]])\n",
    "    y.append([y_[i]])\n",
    "x=np.array(x)\n",
    "y=np.array(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f846ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, 'rx')\n",
    "#plt.scatter(x, y, alpha=0.2, color='red') # use this for scatter plot\n",
    "plt.ylabel(\"AQI\")\n",
    "plt.xlabel(\"PM 2.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8a927",
   "metadata": {},
   "source": [
    "### Minimising Sum of Squared Residuals: Iterative Solution \n",
    "\n",
    "Now we are going to fit a line, $y_i=w_0 + w_1x_i$, to the data you've plotted. We are trying to minimize the error function\n",
    "\n",
    "$$E(w_0, w_1) = \\sum_{i=1}^N(y_i-w_0-w_1x_i)^2$$\n",
    "\n",
    "with respect to $w_0$ and $w_1$. We can start with an initial guess for $w_1$ (why $w_1$?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0a715",
   "metadata": {},
   "source": [
    "Then we use the maximum likelihood update, derived in the lecture to find an estimate for the offset, $w_0$,\n",
    "\n",
    "$$w_0 = \\frac{\\sum_{i=1}^N(y_i-w_1 x_i)}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37746dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = (y - w_1*x).mean()\n",
    "w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c14df1",
   "metadata": {},
   "source": [
    "And now we can make an estimate for the slope of the line, using this estimate of *w_0*,\n",
    "\n",
    "$$w_1 = \\frac{\\sum_{i=1}^N (y_i - w_0) \\times x_i}{\\sum_{i=1}^N x_i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = #.....over to you\n",
    "w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79919e28",
   "metadata": {},
   "source": [
    "We can have a look at how good our fit is by computing the prediction across the input space. First create a vector of 'test points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#range of x-axis of data\n",
    "ll=20\n",
    "ul=300\n",
    "x_test = np.arange(ll, ul)[:, None]\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6092df7",
   "metadata": {},
   "source": [
    "Now use this vector to compute some test predictions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4321a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = ..... #write your code here\n",
    "f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = w_0 + w_1*x #.....write your code here\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0cf90",
   "metadata": {},
   "source": [
    "Now plot those test predictions with a blue line on the same plot as the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27145c9f",
   "metadata": {},
   "source": [
    "Next compute the quality of the fit by evaluating the average sum of squares error of the prediction over the training samples, $E(w_0,w_1)$\n",
    "\n",
    "$$E(w_0, w_1) = \\sum_{i=1}^N(y_i-w_0-w_1x_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS= ((y - w_0 - w_1*x)**2).sum()#.....write your code here \n",
    "RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b034480",
   "metadata": {},
   "source": [
    "The fit isn't very good, we need to iterate between these parameter updates in a loop to improve the fit, we have to do this several times,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(10):\n",
    "    w_1 = ((y - w_0)*x).sum()/(x*x).sum()\n",
    "    w_0 = (y - w_1*x).sum()/y.shape[0]\n",
    "    if i % 10 == 0: \n",
    "        print ('iterations', i, 'training error E', ((y - w_0 - w_1*x)**2).sum())\n",
    "(w_0, w_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832cacf",
   "metadata": {},
   "source": [
    "And let's try plotting the result again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ed3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test =   w_0 + w_1*x_test #.....write your code here \n",
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eac1b0",
   "metadata": {},
   "source": [
    "Does more than 10 iterations considerably improve fit in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeeaef0",
   "metadata": {},
   "source": [
    "### Analytical  Solution (Normal Equation): Direct Solution with Linear Algebra\n",
    "\n",
    "In lecture, we saw that it's possible to solve for the optimal weights $\\mathbf{w}^\\star$ analytically. The solution is (Maximum Likelihood estimate)\n",
    "$$\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "where\n",
    "$$\\mathbf{X} = \\begin{pmatrix} \n",
    "        1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \n",
    "    \\end{pmatrix} \n",
    "  \\quad \\text{and} \\quad \n",
    "  \\mathbf{y} = \\begin{pmatrix} \n",
    "          y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We construct $\\mathbf{X}$ in the code block below, remembering to include the $x_0 = 1$ column for the bias (intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones_like(x), x))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488333f5",
   "metadata": {},
   "source": [
    "To get the solution using Normal eequation, we solve the following system of linear equations:\n",
    "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}^\\star = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "This can be done in numpy using the command `np.linalg.solve`. Dot product can be done using `np.dot`. (Try `np.linalg.solve?` or `np.dot?` in a cell to see what inputs they take\", for transpose of a matrix `X` you can use `X.T`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68df660",
   "metadata": {},
   "outputs": [],
   "source": [
    "w =  #.....write your code here \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d7429",
   "metadata": {},
   "source": [
    "Let's examine the quality of fit for these values for the weights $w_0$ and $w_1$. We create a vector of \"test\" values `x_test` and a function to compute the predictions according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93097e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.arange(ll, ul)[:, None]\n",
    "\n",
    "def predict(x_test, w0, w1): \n",
    "    return  #.....write your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b28f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabf088",
   "metadata": {},
   "source": [
    "Now plot the test predictions with a blue line on the same plot as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(x_test, y_test, x, y): \n",
    "    plt.plot(x_test, y_test, 'b-')\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylabel(\"AQI\")\n",
    "    plt.xlabel(\"PM 2.5\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_fit(x_test, predict(x_test, w0, w1), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175f62e",
   "metadata": {},
   "source": [
    "We'll compute the residual sum of squares $RSS(w_0,w_1)$ on the training set to measure the goodness of fit.\n",
    "\n",
    "Expanding out the RSS for this simple case (where $\\mathbf{w}=[w_0, w_1]$) we have:\n",
    "$$RSS(w_0, w_1) = \\sum_{i=1}^{n}(y_i - w_0 - w_1 x_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_RSS(x, y, w0, w1): \n",
    "    return  #.....write your code here \n",
    "\n",
    "print(compute_RSS(x, y, w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a567131",
   "metadata": {},
   "source": [
    "### Hold Out Validation\n",
    "\n",
    "The error we computed above is the *training* error. It doesn't assess the model's generalization ability, it only assesses how well it's performing on the given training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b301ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "x_train = x\n",
    "y_train = y\n",
    "\n",
    "indices_hold_out=random.sample(range(0,84), 30)\n",
    "#print(indices_hold_out)\n",
    "x_train = np.delete(x, indices_hold_out)[:,None]\n",
    "y_train = np.delete(y, indices_hold_out)[:,None]\n",
    "\n",
    "# Create a hold out set\n",
    "x_hold_out = x[indices_hold_out]\n",
    "y_hold_out = y[indices_hold_out]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339be60d",
   "metadata": {},
   "source": [
    "***Home Exercise:*** Now use the training set and hold out set to compute the errors like above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78133a8f",
   "metadata": {},
   "source": [
    "You might also consider *leave one out* cross validation, where the model is trained *n-1* times with each data point excluded once from training and used for testing, or *k-fold cross validation* in which the data is split into equal size folds and the model is trained *k* times with each fold used once for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5afa6b",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3526335",
   "metadata": {},
   "source": [
    "Now we will consider a more complex polynomial function. Where before we had instances of the form,\n",
    "$$\\phi(\\mathbf{x}) = [ 1~ x ]$$ \n",
    "now we will be using e.g., \n",
    "$$\\phi(\\mathbf{x}) = [ 1 ~x~ x^2~ x^3~ x^4]$$ \n",
    "for a quartic model. Each element $w_i$ of the weight vector corresponds to the coefficient of the input year raised to the $i^{th}$ power. We will consider a range of polynomial models of different orders. \n",
    "\n",
    "To implement this we will use *basis functions* which provide a neat way of representing our data instances such that we can still use all the linear models to achieve learn a non-linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbdac33",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de61b0",
   "metadata": {},
   "source": [
    "The first thing we'll do is plot the training error for the polynomial fit. To do this let's set up some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = x.shape[0]\n",
    "num_pred_data = 30 # how many points to use for plotting predictions\n",
    "x_pred = np.linspace(ll,ul, num_pred_data)[:, None] # input locations for predictions\n",
    "order = 4 # The polynomial order to use.\n",
    "print ('Num of training samples: ',num_data)\n",
    "print('Num of testing samples: ',num_pred_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad533e6a",
   "metadata": {},
   "source": [
    "Now let's build the *basis* matrices $\\Phi$ to represent the training data, where each column is raising the input year $X$ to various powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.zeros((num_data, order+1))\n",
    "Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "for i in range(0, order+1):\n",
    "    Phi[:, i:i+1] =  #.....write your code here \n",
    "    Phi_pred[:, i:i+1] = #.....write your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07747e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a4ce0",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Now we can solve for the regression weights and make predictions both for the training data points, and the test data points. That involves solving the linear system given by\n",
    "\n",
    "$$\\Phi' \\Phi \\mathbf{w} = \\Phi' \\mathbf{y}$$\n",
    "\n",
    "with respect to $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fa9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the linear system\n",
    "w =  #.....write your code here \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1942fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use resulting vector to make predictions at the training points and test points\n",
    "f =  #.....write your code here \n",
    "f_pred = #.....write your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08720a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the residual sum of squares (error)\n",
    "RSS =   (((y-f)**2).sum())  #.....write your code here \n",
    "RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ca4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have the fit and the error, so let's plot the fit and the error.\n",
    "\n",
    "print(\"The error is: %2.4f\"%RSS)\n",
    "plt.plot(x_pred, f_pred)\n",
    "plt.plot(x, y, 'rx')\n",
    "ax = plt.gca()\n",
    "ax.set_title('Predictions for Order 4')\n",
    "ax.set_xlabel('PM 2.5')\n",
    "ax.set_ylabel('AQI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496e281",
   "metadata": {},
   "source": [
    "Now use the loop structure below to compute the error for different model orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the time model to allow python to pause.\n",
    "# import the IPython display module to clear the output.\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "error_list = []\n",
    "max_order = 6\n",
    "fig1=plt.figure(figsize=(15,2*max_order))\n",
    "index=1\n",
    "\n",
    "for order in range(0, max_order+1):\n",
    "    # 1. build the basis set\n",
    "    Phi = np.zeros((num_data, order+1))\n",
    "    Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "    for i in range(0, order+1):\n",
    "        Phi[:, i:i+1] = x**i\n",
    "        Phi_pred[:, i:i+1] = x_pred**i\n",
    "    # 2. solve the linear system\n",
    "    w = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y))\n",
    "\n",
    "    # 3. make predictions at training and test points\n",
    "    f = np.dot(Phi, w)\n",
    "    f_pred = np.dot(Phi_pred, w)\n",
    "    \n",
    "    # 4. compute the training error and append it to a list.\n",
    "    RSS = (((y-f)**2).sum())  \n",
    "    error_list.append(RSS)\n",
    "    \n",
    "    # 5. plot the predictions\n",
    "    fig1.add_subplot(max_order+1,2,index)\n",
    "    plt.plot(x_pred, f_pred)\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylim((2.5, 500))\n",
    "    if (order <7):\n",
    "        plt.title('Predictions for Order ' + str(order) + ' model.')\n",
    "    \n",
    "    \n",
    "    fig1.add_subplot(max_order+1,2,index+1)\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.6)\n",
    "    plt.plot(np.arange(0, order+1), np.asarray(error_list))\n",
    "    plt.xlim((0, order+1))\n",
    "    plt.ylim((0, np.max(error_list)))\n",
    "    if (order ==0):\n",
    "        plt.title('Training Error')\n",
    "    index= index+2\n",
    "\n",
    "plt.show()\n",
    "#display(fig)\n",
    "print('Training error list: ',np.around(error_list,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b621e2",
   "metadata": {},
   "source": [
    "**Home Task:** Looks like a great fit. Does that mean we can stop here, our job is done? You might want to try an order 20 or higher model, also to see if the fits continue to improve with higher order models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb506cd",
   "metadata": {},
   "source": [
    "**Discussion:** What do you think might happen if we try to fit an order 100 model to this data? Is this even a reasonable thing to try?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12174c2b",
   "metadata": {},
   "source": [
    "### Model Generalization using Hold-out Validation\n",
    "The error we computed above is the training error. It doesn't assess the model's generalization ability, it only assesses how well it's performing on the given training data. \n",
    "\n",
    "\n",
    "In hold out validation, we keep back some of the training data for assessing generalization performance. To perform hold out validation, we first remove the hold out set from training data to create train, test (or prediction) or validation set (hold out). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529a9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Create a training set\n",
    "x_train = x\n",
    "y_train = y\n",
    "\n",
    "indices_hold_out=random.sample(range(0,84), 30)\n",
    "#print(indices_hold_out)\n",
    "x_train = np.delete(x, indices_hold_out)[:,None]\n",
    "y_train = np.delete(y, indices_hold_out)[:,None]\n",
    "\n",
    "# Create a hold out set\n",
    "x_hold_out = x[indices_hold_out]\n",
    "y_hold_out = y[indices_hold_out]\n",
    "\n",
    "\n",
    "print ('Whole dataset size', x.shape)\n",
    "print('Train split size: ', x_train.shape)\n",
    "print('Hold-Out split size: ', x_hold_out.shape)\n",
    "\n",
    "# Now use the training set and hold out set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8426f87",
   "metadata": {},
   "source": [
    "Now you have the training and hold out data, you should be able to use the code above to evaluate the model on the hold out data. Do this in the code block below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "error_list = []\n",
    "max_order = 6\n",
    "fig2=plt.figure(figsize=(12,max_order*2))\n",
    "index = 1\n",
    "for order in range(0, max_order+1):\n",
    "    # 1. build the basis set using x_train, x_hold_out, and prediction set\n",
    "    Phi = np.zeros((x_train.shape[0], order+1))\n",
    "    Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "    Phi_hold_out = np.zeros((x_hold_out.shape[0], order+1))\n",
    "    for i in range(0, order+1):\n",
    "        Phi[:, i:i+1] = x_train**i\n",
    "        Phi_hold_out[:, i:i+1] = x_hold_out**i\n",
    "        Phi_pred[:, i:i+1] = x_pred**i\n",
    "        \n",
    "    # 2. solve the linear system\n",
    "    w = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y_train))\n",
    "\n",
    "    # 3. make predictions at training, hold_out, and prediction points\n",
    "    f = np.dot(Phi, w)\n",
    "    f_hold_out = np.dot(Phi_hold_out, w)\n",
    "    f_pred = np.dot(Phi_pred, w)\n",
    "    \n",
    "    # 4. compute the hold out error and append it to a list.\n",
    "    error = (((y_hold_out-f_hold_out)**2).sum())   \n",
    "    error_list.append(error)\n",
    "    \n",
    "    # 5. plot the predictions\n",
    "    fig2.add_subplot(max_order+1,2,index)\n",
    "    plt.plot(x_pred, f_pred)\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylim((2.5, 500))\n",
    "    if (order <7):\n",
    "        plt.title('Predictions for Order ' + str(order) + ' model.')\n",
    "    \n",
    "    fig2.add_subplot(max_order+1,2,index+1)\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.6)\n",
    "    fig2.add_subplot(max_order+1,2,index+1)\n",
    "    plt.plot(np.arange(0, order+1), np.asarray(error_list))\n",
    "    plt.xlim((0, order+1))\n",
    "    plt.ylim((0, np.max(error_list)))\n",
    "    if (order ==0):\n",
    "        plt.title('Hold out Error')\n",
    "    index= index+2\n",
    "\n",
    "plt.show()\n",
    "#display(fig)\n",
    "print('Holdout error list: ', np.around(error_list,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bbb60",
   "metadata": {},
   "source": [
    "**Discussion:** What is going on here? Does this match your earlier findings, or your intuition about which model order was most appropriate? Why isn't hold-out error behaving the same as training error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521fa8d",
   "metadata": {},
   "source": [
    "## Regularizing the Model, Using Ridge Regression\n",
    "\n",
    "A nice way to limit model complexity is *regularisation* where model parameters are penalised from moving to high magnitude values (which means the model is getting overly confident)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e35c8",
   "metadata": {},
   "source": [
    "For this exercise, we'll use a 6th order model, which you might consider much too powerful for this simple problem. As a first step, we'll preprocess the features to ensure they are all operating in a similar range, which means the weights for the 6th order features will take on radically different values to the 1st order features. (Recall that when we regularize, we encourage weights to be small. A weight associated with the higher orders of feature can be smaller than a weight associated with the other (low orders) feature and express the same thing. So, we're not letting the data decide which features to weight more heavily, we've accidentally biased it towards favoring the first to the second.)\n",
    "\n",
    "\n",
    "To correct for this, and allow regularisation with a single constant, we'll normalize (z-score) the columns of training Phi to have zero mean and unit standard deviation. This same transformation is also applied to the testing basis matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 6\n",
    "Phi = np.zeros((x_train.shape[0], order+1))\n",
    "Phi_pred = np.zeros((num_pred_data, order+1))\n",
    "Phi_hold_out = np.zeros((x_hold_out.shape[0], order+1))\n",
    "for i in range(0, order+1):\n",
    "    Phi[:, i:i+1] = x_train**i\n",
    "    if i > 0:\n",
    "        mean = Phi[:, i:i+1].mean()\n",
    "        std = Phi[:, i:i+1].std()\n",
    "        print(i,mean,std)\n",
    "    else: # as the first column is constant, need to avoid divide by zero \n",
    "        mean = 0\n",
    "        std = 1\n",
    "    \n",
    "    Phi[:, i:i+1] = (Phi[:, i:i+1] - mean) / std\n",
    "    Phi_hold_out[:, i:i+1] = (x_hold_out**i - mean) / std\n",
    "    Phi_pred[:, i:i+1] = (x_pred**i - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7965fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we'll perform training, trying out different values of the regularisation coefficient, lambda.\n",
    "\n",
    "error_list = []\n",
    "train_error_list = []\n",
    "lambdas = [1e-10, 1e-6, 1e-4, 1e-2, 1, 10] \n",
    "order = 6\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "fig3=plt.figure(figsize=(16,order*4))\n",
    "index =1\n",
    "for l, lamba in enumerate(lambdas):\n",
    "    # 1. build the basis set using x_train, x_hold_out\n",
    "    # done above\n",
    "        \n",
    "    # 2. solve the linear system\n",
    "    w =  np.linalg.solve(np.dot(Phi.T, Phi) + lamba * np.eye(order+1), np.dot(Phi.T, y_train))#.....write your code here \n",
    "\n",
    "    # 3. make predictions at training and test points\n",
    "    f = np.dot(Phi, w)\n",
    "    f_hold_out = np.dot(Phi_hold_out, w)\n",
    "    f_pred = np.dot(Phi_pred, w)\n",
    "    \n",
    "    # 4. compute the hold and training error and append it to a list.\n",
    "    error = (((y_hold_out-f_hold_out)**2).sum())  \n",
    "    error_list.append(error)\n",
    "    train_error = (((y_train-f)**2).sum())   \n",
    "    train_error_list.append(train_error)\n",
    "    \n",
    "    # 5. plot the predictions\n",
    "    fig3.add_subplot(len(lambdas)+1,3,index)\n",
    "    plt.plot(x_pred, f_pred)\n",
    "    plt.plot(x, y, 'rx')\n",
    "    plt.ylim(2.5, 500)\n",
    "    if (l==0):\n",
    "        plt.title('Pred. for Lambda ' + str(lamba))\n",
    "    else: \n",
    "        plt.title(str(lamba))\n",
    "        \n",
    "    fig3.add_subplot(len(lambdas)+1,3,index+1)\n",
    "    plt.plot(lambdas[:l+1], np.asarray(error_list))\n",
    "    plt.xlim((min(lambdas), max(lambdas)))\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(10000, 70000)\n",
    "    if (l==0):\n",
    "        plt.title('Held-out Error (validation/testing)')\n",
    "    \n",
    "    \n",
    "    fig3.add_subplot(len(lambdas)+1,3,index+2)\n",
    "    plt.plot(lambdas[:l+1], np.asarray(train_error_list))\n",
    "    plt.xlim(min(lambdas), max(lambdas))\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(10000, 50000)\n",
    "    if (l == 0):\n",
    "        plt.title('Training Error')\n",
    "    index= index+3\n",
    "\n",
    "plt.show()\n",
    "#display(fig)\n",
    "print('Holdout error list: ',np.around(error_list,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f7364",
   "metadata": {},
   "source": [
    "**Discussion:** What setting gives the best heldout performance? How does this relate to the training error, and can you describe whether you see evidence of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40056ffa",
   "metadata": {},
   "source": [
    "Now that you have a good understanding of what's going on under the hood in Linear Regression, you can use the functionality in `sklearn` to solve linear regression problems you encounter in the future. Using the `LinearRegression` module, fitting a linear regression model becomes a one-liner as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833aab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try at home\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(x, y)\n",
    "lr.intercept_\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f2680",
   "metadata": {},
   "source": [
    "#### Polynomial basis functions\n",
    "Since the relationship between $y$ and $x$ is non-linear, we'll apply polynomial basis expansion to degree $d$.\n",
    "Specifically, we replace the original data matrix $\\mathbf{X}$ by the transformed matrix $\\mathbf{\\Phi}$, as we have seen before in this workshop. Note that we have to include a column of ones to account for the bias term.\n",
    "\n",
    "The function below is a wrapper around `sklearn.preprocessing.PolynomialFeatures`, which implements the above transformation on a train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931806d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def polynomial_features(X_train, X_test, degree, include_bias=True):\n",
    "    \"\"\"\n",
    "    Augments data matrices X_train and X_test with polynomial features\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=include_bias)\n",
    "    \n",
    "    Phi_train = poly.fit_transform(X_train)\n",
    "    Phi_test = poly.fit_transform(X_test)\n",
    "    \n",
    "    return Phi_train, Phi_test\n",
    "    \n",
    "Phi, Phi_test = polynomial_features(x_train, x_test, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d3cbe",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Pattern Recognition and Machine Learning, Christopher Bishop, New York, Springer,  2006. (Chapter 3)\n",
    "2. Machine learning: A Probabilistic Perspective, Kevin Murphy, MIT Press, 2012. (Chapters 17, 18)\n",
    "3. Statistical Machine Learning Course Workshop, 2015, University of Melbourne, Australia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
