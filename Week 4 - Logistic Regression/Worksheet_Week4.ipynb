{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0284222c",
   "metadata": {},
   "source": [
    "# CP 217, ML4CPS Workshop \n",
    "## Logistic Regression (from scratch) \n",
    "***\n",
    "\n",
    "This worksheet walks you through the process of training & classifying with a logistic regression model. This is to provide you the chance to better understand the working of the model.\n",
    "\n",
    "First, we will understand the Sigmoid function, Hypothesis function, Decision Boundary, the Loss function and code them alongside. After that, we will apply the Gradient Descent Algorithm to find the parameters, weights and bias . Finally, we will measure accuracy and plot the decision boundary for a linearly separable dataset and a non-linearly separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cd315",
   "metadata": {},
   "source": [
    "Let's generate some simple 2d data to demonstrate logistic regression. Note that usually we'll work with more than 2 dimensions, however for the sake of plotting the results we'll stick to 2d data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157caf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=2, n_redundant=0, \n",
    "                           n_informative=2, random_state=1,class_sep=0.5, \n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0779b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[y==0,0], X[y==0,1], 'o')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15c655",
   "metadata": {},
   "source": [
    "We construct $\\mathbf{X}$ in the code block below, remembering to include the $x_0 = 1$ column for the bias (intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c744490",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "For Logistic Regression, our hypothesis is \n",
    "$$\n",
    "\\hat{y} = h_w(x) = \\frac{1}{1+e^{-(w^{T}x)}}\n",
    "$$\n",
    "The output rangeo of $\\hat{y}$ is between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f027e7a",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "\n",
    "Let's first implement a Sigmoid function. \n",
    "\n",
    "The Sigmoid Function squishes all its inputs (values on the x-axis) between 0 and 1.\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc51c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return  #....over to you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66bfc3",
   "metadata": {},
   "source": [
    "As we saw in the lecture, following is the cost fucntion for Logistic Regression for binary classification:\n",
    "$$\n",
    "J(data, w) = \\frac{1}{n}\\sum_{i=1}^{n} L(\\hat{y}^{(i)},y^{(i)}) = -\\frac{1}{n}\\sum_{i=1}^{n} [y^{(i)}log(\\hat{y}^{(i)}) + (1-y^{(i)})log(1-\\hat{y}^{(i)})]\n",
    "$$\n",
    "\n",
    "This loss is also called binary cross entropy error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e09238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    loss =   #....over to you\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea0edd",
   "metadata": {},
   "source": [
    "#### Gradient of the loss function\n",
    "\n",
    "Now that we know our hypothesis function and the loss function, all we need to do is use the Gradient Descent Algorithm to find the optimal values of our parameters like this ($\\eta$ →learning rate), the update rules for parameters are as follows:\n",
    "$$\n",
    "w_{t+1} = w_{t} - \\eta*dw\n",
    "$$\n",
    "Where $dw$ is the partial derivative of loss w.r.t parameter $w$. It looks like:\n",
    "$$\n",
    "dw = \\frac{1}{n} * (\\hat{y}-y).\\textbf{X}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # y_hat --> hypothesis/predictions.\n",
    "    # n-> number of training examples.\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Gradient of loss w.r.t weights.\n",
    "    dw =   #....over to you\n",
    "    \n",
    "    return dw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40da98a",
   "metadata": {},
   "source": [
    "We need to normalize the data before using/computing gradient. It can accelerate the training process.Please make sure we don't normalize the \"bias\" term (first column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ee994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # n-> number of training examples\n",
    "    # d-> number of features \n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Normalizing all the d features of X (except the bias (first) column)\n",
    "    for i in range(d-1):\n",
    "        X[:,i+1] = (X[:,i+1] - X[:,i+1].mean(axis=0))/X[:,i+1].std(axis=0)\n",
    "                \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224af751",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "Now that we have written the functions to learn the parameters, we want to know how our hypothesis($\\hat{y}$) is going to make predictions of whether $y=1$ or $y=0$. The way we defined hypothesis is the probability of $y$ being 1 given $\\textbf{X}$ and parameterized by $w$.\n",
    "\n",
    "So, we will say that it will make a prediction of —\n",
    "$$\n",
    "\\hat{y} = 1 \\to w^{T}\\textbf{X} \\geq 0 \\quad \\text{OR} \\quad \\sigma(w^{T} \\textbf{X}) \\geq 0.5\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = 0  \\to w^{T}\\textbf{X} < 0 \\quad \\text{OR} \\quad  \\sigma(w^{T} \\textbf{X}) < 0.5\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w):\n",
    "    \n",
    "    # X --> Input.\n",
    "    \n",
    "    # Normalizing the inputs.\n",
    "    X = normalize(X)\n",
    "    \n",
    "    # Calculating prediction/y_hat.\n",
    "    preds = #....over to you\n",
    "    \n",
    "    # Empty List to store predictions.\n",
    "    pred_class = []\n",
    "    # if y_hat >= 0.5 --> round up to 1\n",
    "    # if y_hat < 0.5 --> round up to 0\n",
    "    \n",
    "    pred_class =  #....over to you\n",
    "    \n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b49153a",
   "metadata": {},
   "source": [
    "So our decision boundary will be:\n",
    "$$\n",
    "\\hat{y} = 0.5 \\quad or \\quad w^{T}\\textbf{X} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X,w):\n",
    "    ydisp = -(w[0] + w[1] * X)/w[2]\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(X[:, 1][y==0], X[:, 2][y==0], \"g^\")\n",
    "    plt.plot(X[:, 1][y==1], X[:, 2][y==1], \"bs\")\n",
    "    \n",
    "    plt.xlim([-2, 2.2])\n",
    "    plt.ylim([-2, 2.2])\n",
    "    plt.xlabel(\"feature 1\")\n",
    "    plt.ylabel(\"feature 2\")\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.plot(X, ydisp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df4e54",
   "metadata": {},
   "source": [
    "Now that we have written all the required blocks for logistic regression model, let's put them together to train our nodel on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, epochs, eta):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # bs --> Batch Size.\n",
    "    # eta --> Learning rate.\n",
    "        \n",
    "    # n-> number of training examples\n",
    "    # d-> number of features \n",
    "    \n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros((d,1))\n",
    "\n",
    "    \n",
    "    # Reshaping y.\n",
    "    y = y.reshape(n,1)\n",
    "    \n",
    "    # Normalizing the inputs.\n",
    "    X = normalize(X)\n",
    "    \n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "            # Calculating hypothesis/prediction.\n",
    "            y_hat = sigmoid(np.dot(X, w))\n",
    "            \n",
    "            # Getting the gradients of loss w.r.t parameters.\n",
    "            dw = gradients(X, y, y_hat)\n",
    "            \n",
    "            # Updating the parameters.\n",
    "            w -=  #....over to you\n",
    "        \n",
    "              # Calculating loss and appending it in the list.\n",
    "            l = loss(y, sigmoid(np.dot(X, w)))\n",
    "            losses.append(l)\n",
    "        \n",
    "    # returning weights, losses(List).\n",
    "    return w, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "w, l = train(X, y, epochs=100, eta= 0.01)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Decision Boundary\n",
    "plot_decision_boundary(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5543671",
   "metadata": {},
   "source": [
    "What do you think about decision bounday or this model? Let's compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "print('The accuracy of model is',(np.sum(1*(y==predict(X,w)))/len(y))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184dfcd",
   "metadata": {},
   "source": [
    "Are you satistifed with the accuracy on this data? How can we check if there is a scope for improvement or if we did something wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cda24",
   "metadata": {},
   "source": [
    "Can we learn a better model on this data? and How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4aca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, l =  #....over to you   \n",
    "\n",
    "plt.plot(l)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a254fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X, w)\n",
    "print('The accuracy of model is',(np.sum(1*(y==predict(X,w)))/len(y))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159375f",
   "metadata": {},
   "source": [
    "#### Non-linearly separable data\n",
    "\n",
    "Now, we will see the performance of logistic regression for non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24071791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification on non-linearly seperable data\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7abb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[y==0,0], X[y==0,1], 'o')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "w, l = train(X, y, epochs=100, eta=1)\n",
    "# Plotting Decision Boundary\n",
    "plot_decision_boundary(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186440b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy of non-linearly seperable data\n",
    "print('The accuracy of model is',(np.sum(1*(y==predict(X,w)))/len(y))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4594528",
   "metadata": {},
   "source": [
    "### CPS Data: Occupancy Detection based on IoT Sensors\n",
    "\n",
    "Now we will try out logistic regression on a subset of a CPS Domain data. We will be using the Occupancy Detection Data Set from UCI Machine Learning Repository. This is a binary classification problem which requires that an observation of environmental factors such as temperature and humidity be used to classify whether a room is occupied or unoccupied. \n",
    "\n",
    "Data is provided with date-time information and six environmental measures taken each minute over multiple days, specifically:\n",
    "\n",
    "- Data (Timestamp)\n",
    "- Temperature in Celsius.\n",
    "- Relative humidity as a percentage.\n",
    "- Light measured in lux.\n",
    "- Carbon dioxide measured in parts per million.\n",
    "- Humidity ratio, derived from temperature and relative humidity measured in kilograms of water vapor per kilogram of air.\n",
    "- Occupancy as either 1 for occupied or 0 for not occupied.\n",
    "\n",
    "We won't be using time-stamp as a feature in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv file as a pandas dataframe\n",
    "data= pd.read_csv('occupancy_detection.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134e10e",
   "metadata": {},
   "source": [
    "What do you think the best features can be for this classificattion? Let's try predicting using temperature and humidity only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into inputs (features) and output (label)\n",
    "X= data.values[:, 3:5] \n",
    "y= data.values[:, -1].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c027a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[y==0,0], X[y==0,1], 'o')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 's')\n",
    "plt.xlabel(\"temperature\")\n",
    "plt.ylabel(\"humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78709489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.4, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model on the training set\n",
    "model.fit(trainX, trainy)\n",
    "\n",
    "# predict the test set\n",
    "yhat = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, accuracy_score,auc, roc_auc_score\n",
    "\n",
    "print('Precision score %s' % precision_score(testy, yhat))\n",
    "print('Recall score %s' % recall_score(testy, yhat))\n",
    "print('F1-score score %s' % f1_score(testy, yhat))\n",
    "print('Accuracy score %s' % accuracy_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a407bd",
   "metadata": {},
   "source": [
    "Now try with light and $CO_2$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae27442",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Pattern Recognition and Machine Learning, Christopher Bishop, New York, Springer,  2006. (Chapter 3)\n",
    "2. Machine learning: A Probabilistic Perspective, Kevin Murphy, MIT Press, 2012. (Chapters 17, 18)\n",
    "3. Statistical Machine Learning Course Workshop, 2015, University of Melbourne, Australia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
