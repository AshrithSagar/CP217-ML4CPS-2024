\section*{Problem 8}

\textbf{(AdaBoost)}\\
In this problem, we will derive the confidence (influence) \( \alpha_{c} \), which we saw in the class:
\[
    \alpha_{c}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{c}}{\epsilon_{c}}\right)
\]
While this equation might seem a little hard to understand at first glance, we will see that it comes from a simple formulation.
This problem is aimed to walk you through the formulation step-by-step.

Suppose we have a dataset \( \left \{\left(x_{1}, y_{1}\right), \ldots,\left(x_{N}, y_{N}\right)\right \} \) where each \( x_{i} \) has an associated label \( y_{i} \in \{-1,1\} \).
We also have a set of \( L \) weak learners \( \left \{A_{1}, \ldots, A_{L}\right \} \).
Each of the outputs of the classifiers follows \( A_{j}\left(x_{i}\right) \in \{-1,1\} \) for each \( i \in \{1, \ldots, N\}, j \in \) \( \{1, \ldots, L\} \).
Say, after \( (m-1)^{t h} \) iteration, our boosted classifier is a linear combination of weak classifiers of the form:
\[
    C_{(m-1)}\left(x_{i}\right)=\sum_{j=1}^{m-1} \alpha_{j} A_{j}\left(x_{i}\right)
\]

Now, at \( m^{\text {th }} \) iteration, we want to extend this to a better classifier by adding another weak learner \( A_{m} \), with corresponding weight \( \alpha_{m} \).
So, now our goal is to find the optimal \( \left(A_{m}, \alpha_{m}\right) \).
We define a sum of exponential loss for all the datapoints as our loss function for this case:
\[
    \mathcal{L}=\sum_{i=1}^{N} e^{-y_{i} C_{m}\left(x_{i}\right)}
\]
We can simplify the loss function by letting \( w_{i}{ }^{(1)}=1 \) and \( w_{i}{ }^{(m)}=e^{-y_{i} C_{m-1}\left(x_{i}\right)} \)
\begin{enumerate}[label= (\alph*), noitemsep, topsep=0pt]
    \item First, simplify the loss function by noting that \( y_{i} A_{m}\left(x_{i}\right) \in\{-1,1\} \) and split the loss into two terms, namely one for which \( y_{i} A_{m}\left(x_{i}\right)=1 \) and other in which \( y_{i} A_{m}\left(x_{i}\right)=-1 \)

    \item Now, take the derivative of the loss with respect to \( \alpha_{m} \) and setting it to zero, show that:
          \[
              \alpha_{m}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{m}}{\epsilon_{m}}\right)
          \]
          where \( \epsilon_{m}=\frac{\sum_{y_{i} \neq A_{m}\left(x_{i}\right)} w_{i}{ }^{(m)}}{\sum_{i=1}^{N} w_{i}{ }^{(m)}} \)
\end{enumerate}

\subsection*{Solution}
