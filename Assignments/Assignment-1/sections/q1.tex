\section*{Problem 1}

\textbf{(Convexity of the BCE loss)}\\
The Binary Cross Entropy Loss(BCE) for logistic regression is given by:
\[
    \mathcal{L}(\theta)=-\frac{1}{m} \sum_{i=1}^{m} y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)
\]
where \( y^{(i)} \in\{0,1\} \) and \( h_{\theta}(x)=\sigma\left(\theta^{T} x\right), \sigma \) is the usual logistic sigmoid.\\
Find the Hessian \( \mathbf{H} \) of the function and show that it is positive semidefinite.\\
P.S - Positive Semidefiniteness of the Hessian means that the loss function is convex(as mentioned in class), and hence, it will have a unique global minimum.

\subsection*{Solution}

We have the logistic function as
\begin{align*}
    \sigma\left(x\right)
     & =
    \frac{1}{1+e^{-x}}
    \\
    \implies
    \frac{d}{dx} \sigma\left(x\right)
     & =
    -\frac{1}{\left(1+e^{-x}\right)^2} \cdot e^{-x} \cdot (-1)
    =
    \frac{1}{\left(1+e^{-x}\right)}
    \frac{e^{-x}}{\left(1+e^{-x}\right)}
    =
    \sigma\left(x\right) \cdot \left(1-\sigma\left(x\right)\right)
    \\
    \implies
    h_{\theta}\left(x^{(i)}\right)
     & =
    \sigma\left(\theta^{T} x^{(i)}\right)
    =
    \frac{1}{1+e^{-\theta^{T} x^{(i)}}}
\end{align*}
\begin{align*}
    \implies
    \frac{d}{d\theta} \mathcal{L}(\theta)
     & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \left(
    y^{(i)}
    \cdot \frac{1}{\cancel{h_{\theta}\left(x^{(i)}\right)}}
    \cdot \cancel{\sigma\left(\theta^{T} x^{(i)}\right)}
    \cdot \left(1-\sigma\left(\theta^{T} x^{(i)}\right)\right)
    \cdot x^{(i)}
    \right.
    \\ & \qquad
    \left.
    +
    \left(1-y^{(i)}\right)
    \cdot \frac{1}{\cancel{(1-h_{\theta}\left(x^{(i)}\right))}}
    \cdot (-1)
    \cdot \sigma\left(\theta^{T} x^{(i)}\right) \cdot \cancel{\left(1-\sigma\left(\theta^{T} x^{(i)}\right)\right)}
    \cdot x^{(i)}
    \right)
    \\ & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \left(
    y^{(i)} \cdot \left(1-\sigma\left(\theta^{T} x^{(i)}\right)\right) \cdot x^{(i)}
    -
    \left(1-y^{(i)}\right) \cdot \sigma\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}
    \right)
    \\ & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \Big(
    y^{(i)} \cdot x^{(i)}
    -
    \cancel{y^{(i)} \sigma\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}}
    \\ & \qquad \qquad \quad
    -
    \sigma\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}
    +
    \cancel{y^{(i)} \sigma\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}}
    \Big)
    \\ & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \left(
    y^{(i)} \cdot x^{(i)}
    -
    \sigma\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}
    \right)
\end{align*}
The Hessian then is
\begin{align*}
    \implies
    \mathcal{H}(\theta)
     & =
    \frac{d^2}{d\theta^2} \mathcal{L}(\theta)
    =
    -\frac{1}{m} \sum_{i=1}^{m}
    \Big(
    \cancel{\nabla_{\theta} \left(y^{(i)} \cdot x^{(i)}\right)}
    -
    \nabla_{\theta} \left(\sigma\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}\right)
    \Big)
    \\ & =
    \frac{1}{m} \sum_{i=1}^{m}
    \nabla_{\theta} \left(\sigma\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}\right)
    \\ & =
    \frac{1}{m} \sum_{i=1}^{m}
    \sigma'\left(\theta^{T} x^{(i)}\right) \cdot x^{(i)}
    \\ & =
    \frac{1}{m} \sum_{i=1}^{m}
    \sigma\left(\theta^{T} x^{(i)}\right) \cdot \left(1-\sigma\left(\theta^{T} x^{(i)}\right)\right)
    \cdot x^{(i)}
    \cdot x^{(i)}
\end{align*}
\begin{equation*}
    \therefore
    \boxed{
        \mathcal{H}(\theta)
        =
        \frac{1}{m} \sum_{i=1}^{m}
        \sigma\left(\theta^{T} x^{(i)}\right) \cdot \left(1-\sigma\left(\theta^{T} x^{(i)}\right)\right)
        \cdot {\left(x^{(i)}\right)}^2
    }
\end{equation*}
From the above expression, we can see that this sum is non-negative, since the coefficients are all non-negative, since the logistic function is bounded between 0 and 1, and that the square of the feature vector is non-negative.
Hence, \underline{the Hessian is positive semidefinite}, and the loss function is convex.
