\section*{Problem 1}

\textbf{(Convexity of the BCE loss)}\\
The Binary Cross Entropy Loss (BCE) for logistic regression is given by:
\[
    \mathcal{L}(\theta)=-\frac{1}{m} \sum_{i=1}^{m} y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)
\]
where \( y^{(i)} \in \{0,1\} \) and \( h_{\theta}(x)=\sigma\left(\theta^{T} x\right), \sigma \) is the usual logistic sigmoid.\\
Find the Hessian \( \mathbf{H} \) of the function and show that it is positive semidefinite.\\
P.S --- Positive Semidefiniteness of the Hessian means that the loss function is convex(as mentioned in class), and hence, it will have a unique global minimum.

\subsection*{Solution}

The usual logistic function is given by
\begin{align*}
    \sigma\left(t\right)
     & =
    \frac{1}{1+e^{-t}}
    \\
    \implies
    \frac{d}{dt} \sigma\left(t\right)
     & =
    -\frac{1}{{\left(1+e^{-t}\right)}^2} \cdot e^{-t} \cdot (-1)
    =
    \frac{1}{\left(1+e^{-t}\right)}
    \frac{e^{-t}}{\left(1+e^{-t}\right)}
    =
    \sigma\left(t\right) \cdot \left(1-\sigma\left(t\right)\right)
    \\
    \implies
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)
     & =
    \sigma\left(\boldsymbol{\theta}^{T} \mathbf{x}^{(i)}\right)
    =
    \frac{1}{1+e^{-\boldsymbol{\theta}^{T} \mathbf{x}^{(i)}}}
    \\
    \implies
    \frac{\partial}{\partial \boldsymbol{\theta}} h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)
     & =
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right) \mathbf{x}^{(i)}
\end{align*}
Now, with the loss function as above,
\begin{equation*}
    \mathcal{L}(\boldsymbol{\theta})
    =
    -\frac{1}{m} \sum_{i=1}^{m}
    y^{(i)} \log \left(h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    +
    \left(1-y^{(i)}\right) \log \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
\end{equation*}
the derivative of the loss function is
\begin{align*}
    \implies
    \frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})
     & =
    -\frac{1}{m} \sum_{i=1}^{m}
    y^{(i)} \frac{\partial}{\partial \boldsymbol{\theta}} \log \left(h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    +
    \left(1-y^{(i)}\right) \frac{\partial}{\partial \boldsymbol{\theta}} \log \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    \\ & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \left(
    y^{(i)}
    \cdot \frac{1}{\cancel{h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)}}
    \cdot \cancel{h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)}
    \cdot \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    \mathbf{x}^{(i)}
    \right.
    \\ & \qquad
    \left.
    +
    \left(1-y^{(i)}\right)
    \cdot \frac{1}{\cancel{(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right))}}
    \cdot (-1)
    \cdot h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)
    \cdot \cancel{\left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)}
    \mathbf{x}^{(i)}
    \right)
\end{align*}
\begin{align*}
     & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \left(
    y^{(i)} \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right) \mathbf{x}^{(i)}
    -
    \left(1-y^{(i)}\right) h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \mathbf{x}^{(i)}
    \right)
    \\ & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \Big(
    y^{(i)} \mathbf{x}^{(i)}
    -
    \cancel{y^{(i)} h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \mathbf{x}^{(i)}}
    -
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \mathbf{x}^{(i)}
    +
    \cancel{y^{(i)} h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \mathbf{x}^{(i)}}
    \Big)
    \\ & =
    -\frac{1}{m} \sum_{i=1}^{m}
    \left(
    y^{(i)} \mathbf{x}^{(i)}
    -
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \mathbf{x}^{(i)}
    \right)
    \\
    \implies
    \frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})
     & =
    \frac{1}{m} \sum_{i=1}^{m}
    \left(
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)
    -
    y^{(i)}
    \right)
    \mathbf{x}^{(i)}
\end{align*}

The Hessian then is
\begin{align*}
    \implies
    \mathbf{H}(\boldsymbol{\theta})
     & =
    \frac{\partial^2}{\partial \boldsymbol{\theta}^2} \mathcal{L}(\boldsymbol{\theta})
    =
    \frac{1}{m} \sum_{i=1}^{m}
    \left(
    \frac{\partial}{\partial \boldsymbol{\theta}} h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \mathbf{x}^{(i)}
    -
    \cancel{\frac{\partial}{\partial \boldsymbol{\theta}} y^{(i)} \mathbf{x}^{(i)}}
    \right)
    \\ & =
    \frac{1}{m} \sum_{i=1}^{m}
    \left( \frac{\partial}{\partial \boldsymbol{\theta}} h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \right) \mathbf{x}^{(i)}
    =
    \frac{1}{m} \sum_{i=1}^{m}
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right) \mathbf{x}^{(i)} {\mathbf{x}^{(i)}}^\top
\end{align*}
\begin{equation*}
    \therefore
    \boxed{
    \mathbf{H}(\boldsymbol{\theta})
    =
    \frac{1}{m} \sum_{i=1}^{m}
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    \mathbf{x}^{(i)} {\mathbf{x}^{(i)}}^\top
    }
\end{equation*}
From the above expression, we can see that this sum is non-negative, since the coefficients are all non-negative, since the logistic function is bounded between 0 and 1, and that the square function is non-negative, thereby \( \mathbf{H}(\boldsymbol{\theta}) \) is in a positive semidefinite quadratic form.
Hence, \underline{the Hessian is positive semidefinite}, and the loss function is convex, and has a unique global minimum.

That is, for any vector \( \mathbf{v} \in \mathbb{R}^n \), the quadratic form \( \mathbf{v}^\top \mathbf{H}(\boldsymbol{\theta}) \mathbf{v} \geq 0 \), since
\begin{align*}
    \mathbf{v}^\top \mathbf{H}(\boldsymbol{\theta}) \mathbf{v}
     & =
    \frac{1}{m} \sum_{i=1}^{m}
    \mathbf{v}^\top
    \left(
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    \mathbf{x}^{(i)} {\mathbf{x}^{(i)}}^\top
    \right)
    \mathbf{v}
    \\ & =
    \frac{1}{m} \sum_{i=1}^{m}
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    \mathbf{v}^\top
    \mathbf{x}^{(i)} {\mathbf{x}^{(i)}}^\top
    \mathbf{v}
    \\ & =
    \frac{1}{m} \sum_{i=1}^{m}
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    \left(\mathbf{v}^\top \mathbf{x}^{(i)}\right) {\left(\mathbf{v}^\top \mathbf{x}^{(i)}\right)}^\top
    \\ & =
    \frac{1}{m} \sum_{i=1}^{m}
    h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right)
    {\left(\mathbf{v}^\top \mathbf{x}^{(i)}\right)}^2
\end{align*}
and that the coefficients \( h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right) \left(1-h_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\right) \geq 0 \), and that \( {\left(\mathbf{v}^\top \mathbf{x}^{(i)}\right)}^2 \geq 0 \), thereby the quadratic form is non-negative, and the Hessian is positive semidefinite.
