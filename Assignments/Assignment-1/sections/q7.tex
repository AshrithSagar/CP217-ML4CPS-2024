\section*{Problem 7}

\textbf{(Support Vector Machines-II:\@ \( l_{2}
    \) norm soft margin SVM)}\\
The \( l_{2} \) norm soft margin SVM algorithm is given by the following optimization problem:
\[
    \min _{w, b, \xi} \quad \frac{1}{2}\|w\|^{2}+\frac{C}{2} \sum_{i=1}^{m} \xi_{i}^{2} \quad \text { s.t } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1-\xi_{i}, i=1, \ldots, m
\]
\begin{enumerate}[label= (\alph*), noitemsep, topsep=0pt]
    \item Notice that the constrains \( \xi_{i} \geq 0 \) have been dropped.
          Show that these nonnegativity constraints can be removed.
          That is, the optimal value of the objective will be the same whether or not these constraints are present.

    \item What is the Lagrangian of the \( l_{2} \) norm soft margin SVM optimization problem?

    \item Minimize the Lagrangian with respect to \( w, b \), and \( \xi \).

    \item What is the dual of the \( l_{2} \) norm soft margin SVM optimization problem?
\end{enumerate}

\subsection*{Solution}

\subsubsection*{(a) The nonnegativity constraints \( \xi_{i} \geq 0 \) can be removed}

Firstly, since the objective function is quadratic in terms of \( \xi_{i} \), the condition \( \xi_{i} \geq 0 \) doesn't matter here, since the square of a real number is always nonnegative.
In the constraints, we can see that if \( \xi_{i} \) were allowed to be negative, then we have that \( 1 - \xi_{i} \) would be greater than 1, and it would be easier to satisfy the constraint \( y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1-\xi_{i} \) for some points.
Thereby, the optimisation problem would naturally choose a positive value for \( \xi_{i} \) to satisfy the constraints, and therby the nonnegativity constraints \( \xi_{i} \geq 0 \) can be removed here.

\subsubsection*{(b) The Lagrangian of the \( l_{2} \) norm soft margin SVM optimization problem}

The Lagrangian \( \mathcal{L} \) of the \( l_{2} \) norm soft margin SVM optimization problem is given by, with \( \alpha_{i} \geq 0, \quad i = 1, \ldots, m \quad \) as the Lagrange multipliers,
\begin{align*}
    \mathcal{L}
     & =
    \frac{1}{2}\|w\|^{2}+\frac{C}{2} \sum_{i=1}^{m} \xi_{i}^{2}
    - \sum_{i=1}^{m} \alpha_{i} \left[ y^{(i)}\left(w^{T} x^{(i)}+b\right) - (1-\xi_{i}) \right]
    \\ & =
    \frac{1}{2}\|w\|^{2}+\frac{C}{2} \sum_{i=1}^{m} \xi_{i}^{2}
    - \sum_{i=1}^{m} \alpha_{i} \left[ y^{(i)}\left(w^{T} x^{(i)}+b\right) - 1 + \xi_{i} \right]
    \\ & =
    \frac{1}{2}\|w\|^{2}+\frac{C}{2} \sum_{i=1}^{m} \xi_{i}^{2}
    - \sum_{i=1}^{m} \alpha_{i} y^{(i)} \left(w^{T} x^{(i)}+b\right)
    + \sum_{i=1}^{m} \alpha_{i} - \sum_{i=1}^{m} \alpha_{i} \xi_{i}
    \\
    \implies
    \mathcal{L}
     & =
    \frac{1}{2} \|w\|^{2}
    + \frac{C}{2} \sum_{i=1}^{m} \xi_{i}^{2}
    - \sum_{i=1}^{m} \alpha_{i} y^{(i)} w^{T} x^{(i)}
    - \sum_{i=1}^{m} \alpha_{i} y^{(i)} b
    + \sum_{i=1}^{m} \alpha_{i}
    - \sum_{i=1}^{m} \alpha_{i} \xi_{i}
\end{align*}

\subsubsection*{(c) Minimizing the Lagrangian with respect to \( w, b \), and \( \xi \)}

With respect to \( w \), \( b \), and \( \xi \), the partial derivatives of the Lagrangian \( \mathcal{L} \) need to be set to zero, and we get
\begin{align*}
    \implies
    \nabla_{w} \mathcal{L}
     & =
    w - \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}
    = 0
    \implies w = \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}
    \\
    \implies
    \frac{\partial \mathcal{L}}{\partial b}
     & =
    - \sum_{i=1}^{m} \alpha_{i} y^{(i)}
    = 0
    \implies \sum_{i=1}^{m} \alpha_{i} y^{(i)} = 0
    \\
    \implies
    \nabla_{\xi} \mathcal{L}
     & =
    C \xi - \alpha
    = 0
    \implies \alpha = C \xi
\end{align*}
where \( \alpha = {\left[ \alpha_{1}, \ldots, \alpha_{m} \right]}^\top \) and \( \xi = {\left[ \xi_{1}, \ldots, \xi_{m} \right]}^\top \).

Hence, the conditions to minimise the Lagrangian with respect to \( w, b, \) and \( \xi \) are
\begin{align*}
     &
    w
    =
    \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}
    \\ &
    \sum_{i=1}^{m} \alpha_{i} y^{(i)}
    =
    0
    \\ &
    \xi
    =
    \frac{1}{C} \alpha
\end{align*}

\subsubsection*{(d) The dual of the \( l_{2} \) norm soft margin SVM optimization problem}

The dual problem can be derived by substituting the expressions for \( w \) and \( \xi \) as obtained in part (c) into the Lagrangian \( \mathcal{L} \) and eliminating the primal variables \( w \), \( b \), and \( \xi \).
\begin{align*}
    \implies
    \mathcal{L}
     & =
    \frac{1}{2} w^\top w
    + \frac{C}{2} \sum_{i=1}^{m} \xi_{i}^{2}
    - \sum_{i=1}^{m} \alpha_{i} y^{(i)} w^{T} x^{(i)}
    - \cancel{ \sum_{i=1}^{m} \alpha_{i} y^{(i)} b }
    + \sum_{i=1}^{m} \alpha_{i}
    - \sum_{i=1}^{m} \alpha_{i} \xi_{i}
    \\ & =
    \frac{1}{2}
    \sum_{i=1}^{m} \alpha_{i} y^{(i)} {x^{(i)}}^\top
    \sum_{j=1}^{m} \alpha_{j} y^{(j)} x^{(j)}
    + \frac{C}{2}
    \sum_{i=1}^{m} {\left( \frac{1}{C} \alpha_{i} \right)}^2
    \\ & \qquad \qquad
    - \sum_{i=1}^{m} \alpha_{i} y^{(i)}
    \sum_{j=1}^{m} \alpha_{j} y^{(j)} {x^{(j)}}^\top x^{(i)}
    + \sum_{i=1}^{m} \alpha_{i}
    - \sum_{i=1}^{m} \alpha_{i} \left( \frac{1}{C} \alpha_{i} \right)
    \\ & =
    \frac{1}{2}
    \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y^{(i)} y^{(j)} x^{(i) \top} x^{(j)}
    + \frac{1}{2C}
    \sum_{i=1}^{m} \alpha_{i}^{2}
    \\ & \qquad \qquad
    - \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y^{(i)} y^{(j)} x^{(i) \top} x^{(j)}
    + \sum_{i=1}^{m} \alpha_{i}
    - \frac{1}{C}
    \sum_{i=1}^{m} \alpha_{i}^{2}
    \\
    \implies
    \mathcal{L}
     & =
    \sum_{i=1}^{m} \alpha_{i}
    - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y^{(i)} y^{(j)} x^{(i) \top} x^{(j)}
    - \frac{1}{2C}
    \sum_{i=1}^{m} \alpha_{i}^{2}
\end{align*}

Thereby, the dual of the \( l_{2} \) norm soft margin SVM optimization problem is given by
\begin{align*}
    \max_{\alpha}
    \quad &
    \sum_{i=1}^{m} \alpha_{i}
    - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y^{(i)} y^{(j)} x^{(i) \top} x^{(j)}
    \\
    \text{s.t.}
    \quad &
    0 \leq \alpha_{i} \leq C
    , \quad i = 1, \ldots, m
    \\ &
    \sum_{i=1}^{m} \alpha_{i} y^{(i)} = 0
\end{align*}
