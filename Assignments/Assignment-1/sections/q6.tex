\section*{Problem 6}

\textbf{(Support Vector Machines-I)}
\begin{enumerate}[label= (\alph*), noitemsep, topsep=0pt]
    \item Show that irrespective of the dimensionality of the data space, a data set consisting of just two data points, one form each class, is sufficient to determine the location of the maximum-margin classifier.

    \item Show that the inverse of the square of the magnitude of the optimal margin in an SVM is the sum of all optimal Lagrange multipliers.

    \item Suppose there are 10000 datapoints with two classes in the dataset on which an SVM is run. There are 2000 slacks that are more than unity and 400 nonnegative Lagrange multipliers. What is the training accuracy and how many support vectors are there in this case?
\end{enumerate}

\subsection*{Solution}

\subsubsection*{(a) Location of the maximum-margin classifier with just two data points}

This follows from the fact that we can use the perpendicular bisector between the two points from two different classes, and it is possible to see that the optimal matgin classifier should pass through the midpoint of the line joining the two point from each of the classes.
This is because the margin is maximised when the decision boundary is equidistant from the two points.
In the weighted case, we have a similar result.

\subsubsection*{(b) Inverse of the square of the magnitude of the optimal margin}

The SVM algorithm tries to find the maximum-margin classifier by solving the optimization problem
\begin{equation*}
    \min_{w, b} \frac{1}{2} \| w \|^{2}
    \quad \text{s.t.} \quad
    y^{(i)} \left( w^{T} x^{(i)} + b \right) \geq 1
    \quad
    \forall i = 1, \ldots, m
\end{equation*}
The Lagrangian \( \mathcal{L} \) of the SVM optimization problem is given by
\begin{equation*}
    \mathcal{L} = \frac{1}{2} \| w \|^{2} - \sum_{i=1}^{m} \alpha_{i} \left[ y^{(i)} \left( w^{T} x^{(i)} + b \right) - 1 \right]
\end{equation*}
where \( \alpha_{i} \geq 0 \) are the Lagrange multipliers.
By the KKT conditions, the conditions for optimality are given by
\begin{align*}
    \nabla_{w} \mathcal{L}
     & =
    w - \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)} = 0
    \implies
    w = \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}
    \\
    \nabla_{b} \mathcal{L}
     & =
    - \sum_{i=1}^{m} \alpha_{i} y^{(i)} = 0
    \implies
    \sum_{i=1}^{m} \alpha_{i} y^{(i)} = 0
\end{align*}
\begin{align*}
    \implies
    \lVert w \rVert^{2}
     & =
    \left\lVert \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)} \right\rVert^{2}
\end{align*}
\begin{align*}
    \implies
    \frac{1}{\lVert w \rVert^{2}}
    =
    \sum_{i=1}^{m} \alpha_{i}
\end{align*}

\subsubsection*{(c) Training accuracy and number of support vectors}

The number of support vectors is given by the number of non-zero Lagrange multipliers.
In this case, there are 400 support vectors.
The training accuracy is defined as the proportion of correctly classified data points.
Since there are 2,000 slack variables greater than 1, these correspond to the data points that are either misclassified or violate the margin by more than unity.
The training accuracy then, is given by
\begin{equation*}
    \text{Training Accuracy} = \frac{10000 - 2000}{10000} = 0.8
    \implies
    80\%
\end{equation*}
