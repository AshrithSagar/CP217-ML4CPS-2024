\section*{Problem 6}

\textbf{(Support Vector Machines-I)}

(a) Show that irrespective of the dimensionality of the data space, a data set consisting of just two data points, one form each class, is sufficient to determine the location of the maximum-margin classifier.\\
(b) Show that the inverse of the square of the magnitude of the optimal margin in an SVM is the sum of all optimal Lagrange multipliers.\\
(c) Suppose there are 10000 datapoints with two classes in the dataset on which an SVM is run. There are 2000 slacks that are more than unity and 400 nonnegative Lagrange multipliers. What is the training accuracy and how many support vectors are there in this case?

\subsection*{Solution}
