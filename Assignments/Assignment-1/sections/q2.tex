\section*{Problem 2}

\textbf{(Locally weighted linear regression)}\\
Consider a linear regression problem in which we want to weight different training examples differently.
Specifically, suppose we want to minimize
\begin{equation*}
    \mathcal{L}(\theta)=\frac{1}{2} \sum_{i=1}^{m} w^{(i)}{\left(\theta^{T} x^{(i)}-y^{(i)}\right)}^{2}
\end{equation*}
here \( w^{(i)} \) is the weight given to \( i^{\text {th }} \) training example.

\begin{enumerate}[label= (\alph*), noitemsep, topsep=0pt]
    \item Show that the loss \( \mathcal{L}(\theta) \) can be written as
          \[
              \mathcal{L}(\theta)=(X \theta-y)^{T} W(X \theta-y)
          \]
          for an appropriate weight matrix W, where X and y are the data matrix and label matrix, respectively.

    \item  By finding the derivative \( \nabla_{\theta} \mathcal{L}(\theta) \), generalize the normal equation to weighted setting, and give the value of \( \theta \) that minimizes \( \mathcal{L}(\theta) \).

    \item  Suppose we have a dataset \( \left \{ \left(x^{(i)}, y^{(i)}\right) ; i=1, \ldots m\right \} \) of \( m \) independent examples, but we model \( y^{(i)} \)'s as drawn from a conditional distribution with different levels of variance \( {\left(\sigma^{(i)}\right)}^{2} \). Specifically, assume the model:
          \[
              p\left(y^{(i)} \mid x^{(i)} ; \theta\right)=\frac{1}{\sqrt{2 \pi} \sigma^{(i)}} \exp \left(-\frac{{\left(y^{(i)}-\theta^{T} x^{(i)}\right)}^{2}}{2{\left(\sigma^{(i)}\right)}^{2}}\right)
          \]
          Show that finding the maximum likelihood estimate of \( \theta \) reduces to solving a weighted linear regression problem. State clearly what are \( w^{(i)} \)'s are in terms of \( \sigma^{(i)} \)'s.
\end{enumerate}

\subsection*{Solution}

\subsubsection*{(a) Loss \( \mathcal{L}(\boldsymbol{\theta}) \)}

Given the loss function
\begin{equation*}
    \mathcal{L}(\boldsymbol{\theta})
    =
    \frac{1}{2}
    \sum_{i=1}^{m}
    w^{(i)}{\left(\boldsymbol{\theta}^{\top} \mathbf{x}^{(i)}-y^{(i)}\right)}^{2}
\end{equation*}
we have the data matrix \( \mathbf{X}_{m \times n} \), the label vector \( \mathbf{y}_{m \times 1} \) and the parameter vector \( \boldsymbol{\theta}_{n \times 1} \) as
\begin{equation*}
    \mathbf{X}
    =
    \begin{bmatrix}
        {\left(\mathbf{x}^{(1)}\right)}^\top \\
        {\left(\mathbf{x}^{(2)}\right)}^\top \\
        \vdots                               \\
        {\left(\mathbf{x}^{(m)}\right)}^\top
    \end{bmatrix}
    , \mathbf{x}^{(i)} \in \mathbb{R}^{n}
    , \qquad
    \mathbf{y}
    =
    \begin{bmatrix}
        y^{(1)} \\
        y^{(2)} \\
        \vdots  \\
        y^{(m)}
    \end{bmatrix}
    , y^{(i)} \in \mathbb{R}
    , \qquad
    \boldsymbol{\theta}
    =
    \begin{bmatrix}
        \theta_{1} \\
        \theta_{2} \\
        \vdots     \\
        \theta_{n}
    \end{bmatrix}
    , \theta_{j} \in \mathbb{R}
\end{equation*}
We can then see that
\begin{equation*}
    (\mathbf{X} \boldsymbol{\theta} - \mathbf{y})
    =
    \begin{bmatrix}
        {\left(\mathbf{x}^{(1)}\right)}^\top \boldsymbol{\theta}
        - y^{(1)}
        \\
        {\left(\mathbf{x}^{(2)}\right)}^\top \boldsymbol{\theta}
        - y^{(2)}
        \\
        \vdots
        \\
        {\left(\mathbf{x}^{(m)}\right)}^\top \boldsymbol{\theta}
        - y^{(m)}
    \end{bmatrix}
\end{equation*}
and with a weight matrix \( \mathbf{W}_{m \times m} \) defined as
\begin{equation*}
    \mathbf{W}_{m \times m}
    =
    \begin{bmatrix}
        w^{(1)} & 0       & \cdots & 0       \\
        0       & w^{(2)} & \cdots & 0       \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        0       & 0       & \cdots & w^{(m)}
    \end{bmatrix}
\end{equation*}
\begin{align*}
    \implies
     &
    {(\mathbf{X} \boldsymbol{\theta} - \mathbf{y})}^\top \mathbf{W} (\mathbf{X} \boldsymbol{\theta} - \mathbf{y})
    \\ & =
    \begin{bmatrix}
        {\left(\mathbf{x}^{(1)}\right)}^\top \boldsymbol{\theta} - y^{(1)} \\
        {\left(\mathbf{x}^{(2)}\right)}^\top \boldsymbol{\theta} - y^{(2)} \\
        \vdots                                                             \\
        {\left(\mathbf{x}^{(m)}\right)}^\top \boldsymbol{\theta} - y^{(m)}
    \end{bmatrix}^\top
    \begin{bmatrix}
        w^{(1)} & 0       & \cdots & 0       \\
        0       & w^{(2)} & \cdots & 0       \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        0       & 0       & \cdots & w^{(m)}
    \end{bmatrix}
    \begin{bmatrix}
        {\left(\mathbf{x}^{(1)}\right)}^\top \boldsymbol{\theta} - y^{(1)} \\
        {\left(\mathbf{x}^{(2)}\right)}^\top \boldsymbol{\theta} - y^{(2)} \\
        \vdots                                                             \\
        {\left(\mathbf{x}^{(m)}\right)}^\top \boldsymbol{\theta} - y^{(m)}
    \end{bmatrix}
    \\ & =
    \begin{bmatrix}
        {\left(\mathbf{x}^{(1)}\right)}^\top \boldsymbol{\theta} - y^{(1)} \\
        {\left(\mathbf{x}^{(2)}\right)}^\top \boldsymbol{\theta} - y^{(2)} \\
        \vdots                                                             \\
        {\left(\mathbf{x}^{(m)}\right)}^\top \boldsymbol{\theta} - y^{(m)}
    \end{bmatrix}^\top
    \begin{bmatrix}
        w^{(1)}{\left({\left(\mathbf{x}^{(1)}\right)}^\top \boldsymbol{\theta} - y^{(1)}\right)} \\
        w^{(2)}{\left({\left(\mathbf{x}^{(2)}\right)}^\top \boldsymbol{\theta} - y^{(2)}\right)} \\
        \vdots                                                                                   \\
        w^{(m)}{\left({\left(\mathbf{x}^{(m)}\right)}^\top \boldsymbol{\theta} - y^{(m)}\right)}
    \end{bmatrix}
    \\ & =
    \Bigg [
        w^{(1)} {\left({\left(\mathbf{x}^{(1)}\right)}^\top \boldsymbol{\theta} - y^{(1)}\right)}^\top \left({\left(\mathbf{x}^{(1)}\right)}^\top \boldsymbol{\theta} - y^{(1)}\right)
    \\ & \qquad +
        w^{(2)} {\left({\left(\mathbf{x}^{(2)}\right)}^\top \boldsymbol{\theta} - y^{(2)}\right)}^\top \left({\left(\mathbf{x}^{(2)}\right)}^\top \boldsymbol{\theta} - y^{(2)}\right)
        + \cdots
    \\ & \qquad +
        w^{(m)} {\left({\left(\mathbf{x}^{(m)}\right)}^\top \boldsymbol{\theta} - y^{(m)}\right)}^\top \left({\left(\mathbf{x}^{(m)}\right)}^\top \boldsymbol{\theta} - y^{(m)} \right)
        \Bigg ]
    \\ & =
    \sum_{i=1}^{m}
    w^{(i)} {\left({\left(\mathbf{x}^{(i)}\right)}^\top \boldsymbol{\theta} - y^{(i)}\right)}^\top \left({\left(\mathbf{x}^{(i)}\right)}^\top \boldsymbol{\theta} - y^{(i)} \right)
    =
    \sum_{i=1}^{m}
    w^{(i)} {\left(\boldsymbol{\theta}^\top \mathbf{x}^{(i)}-y^{(i)}\right)}^{2}
    =
    \mathcal{L}(\boldsymbol{\theta})
\end{align*}
since \( \left({\left(\mathbf{x}^{(i)}\right)}^\top \boldsymbol{\theta} - y^{(i)} \right) \) is a scalar, and hence \( {\left(\mathbf{x}^{(i)}\right)}^\top \boldsymbol{\theta} = \boldsymbol{\theta}^\top \mathbf{x}^{(i)} \).

Hence, we have the result, that \( \boxed{\mathcal{L}(\boldsymbol{\theta}) = {(\mathbf{X} \boldsymbol{\theta} - \mathbf{y})}^\top \mathbf{W} (\mathbf{X} \boldsymbol{\theta} - \mathbf{y})} \).

\clearpage
\subsubsection*{(b) Generalized normal equation}

The gradient of the loss function \( \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}) \) is
\begin{align*}
    \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})
    =
    \frac{\partial}{\partial \boldsymbol{\theta}}
    \left(
    \frac{1}{2}
    \sum_{i=1}^{m}
    w^{(i)} {\left(\boldsymbol{\theta}^\top \mathbf{x}^{(i)}-y^{(i)}\right)}^{2}
    \right)
    =
    \sum_{i=1}^{m}
    w^{(i)} \mathbf{x}^{(i)} \left(\boldsymbol{\theta}^\top \mathbf{x}^{(i)}-y^{(i)}\right)
    =
    \mathbf{X}^\top \mathbf{W} (\mathbf{X} \boldsymbol{\theta} - \mathbf{y})
\end{align*}
where \( \mathbf{W} \) is the diagonal matrix with the weights \( w^{(i)} \) on the diagonal.

The generalized normal equation then can be obtained by setting the gradient to zero, and we have
\begin{align*}
     &
    \mathbf{X}^\top \mathbf{W} \mathbf{X} \boldsymbol{\theta} - \mathbf{X}^\top \mathbf{W} \mathbf{y}
    =
    0
    \\
    \implies
     &
    \boxed{
        \mathbf{X}^\top \mathbf{W} \mathbf{X} \boldsymbol{\theta}
        =
        \mathbf{X}^\top \mathbf{W} \mathbf{y}
    }
\end{align*}
Solving this equation for \( \boldsymbol{\theta} \) gives the maximum likelihood estimate of \( \boldsymbol{\theta} \) in a weighted linear regression problem.

\subsubsection*{(c) Maximum likelihood estimate of \( \boldsymbol{\theta} \)}

Given the model
\begin{equation*}
    p\left(y^{(i)} \mid \mathbf{x}^{(i)} ; \boldsymbol{\theta}\right)=\frac{1}{\sqrt{2 \pi} \sigma^{(i)}} \exp \left(-\frac{{\left(y^{(i)}-\boldsymbol{\theta}^{\top} \mathbf{x}^{(i)}\right)}^{2}}{2{\left(\sigma^{(i)}\right)}^{2}}\right)
\end{equation*}
where \( \sigma^{(i)} \) is the variance for the \( i \)th sample.

The likelihood function then, given that the samples are independent, can be expressed as the product of the likelihoods of individual samples, and we have
\begin{align*}
    \mathcal{L}(\boldsymbol{\theta})
     & =
    \prod_{i=1}^{m}
    p\left(y^{(i)} \mid \mathbf{x}^{(i)} ; \boldsymbol{\theta}\right)
    \\ & =
    \prod_{i=1}^{m}
    \frac{1}{\sqrt{2 \pi} \sigma^{(i)}} \exp \left(-\frac{{\left(y^{(i)}-\boldsymbol{\theta}^{\top} \mathbf{x}^{(i)}\right)}^{2}}{2{\left(\sigma^{(i)}\right)}^{2}}\right)
    \\ & =
    \frac{1}{\left(2 \pi\right)^{m/2} \prod_{i=1}^{m} \sigma^{(i)}}
    \exp \left(
    -\frac{1}{2}
    \sum_{i=1}^{m}
    \frac{{\left(y^{(i)}-\boldsymbol{\theta}^{\top} \mathbf{x}^{(i)}\right)}^{2}}{{\left(\sigma^{(i)}\right)}^{2}}
    \right)
    \\
    \implies
    \log \mathcal{L}(\boldsymbol{\theta})
     & =
    -\frac{m}{2} \log(2 \pi) - \sum_{i=1}^{m} \log \left( \sigma^{(i)} \right)
    -\frac{1}{2}
    \sum_{i=1}^{m}
    \frac{{\left(y^{(i)}-\boldsymbol{\theta}^{\top} \mathbf{x}^{(i)}\right)}^{2}}{{\left(\sigma^{(i)}\right)}^{2}}
    \\
    \implies
    \frac{\partial}{\partial \boldsymbol{\theta}} \log \mathcal{L}(\boldsymbol{\theta})
     & =
    \frac{1}{2}
    \sum_{i=1}^{m}
    \frac{2}{\left(\sigma^{(i)}\right)^{2}}
    \left(y^{(i)}-\boldsymbol{\theta}^{\top} \mathbf{x}^{(i)}\right)
    \mathbf{x}^{(i)}
    =
    \sum_{i=1}^{m}
    \frac{1}{\left(\sigma^{(i)}\right)^{2}}
    \left(y^{(i)}-\boldsymbol{\theta}^{\top} \mathbf{x}^{(i)}\right)
    \mathbf{x}^{(i)}
\end{align*}
which is the same as the gradient of the loss function in part (b) with \( w^{(i)} = \cfrac{1}{\left(\sigma^{(i)}\right)^{2}} \).
