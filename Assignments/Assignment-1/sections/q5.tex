\section*{Problem 5}

\textbf{(Neural Networks-II:\@ A simple Neural Network)}\\
Let \( X=\left \{x^{(1)}, \ldots, x^{(m)}\right \} \) be a dataset of m samples with two features, i.e., \( x^{(i)} \in \mathbb{R}^{2} \).
The samples are classified into two categories with label \( y^{(i)} \in \{0,1\} \).
A scatter plot of the dataset is shown in Figure~\ref{fig:q5-1}.
The examples in class 1 is marked ``x'' and examples in class 0 are marked ``o''.
We want to perform binary classification using a simple neural network with the architecture shown in Figure~\ref{fig:q5-2}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/images/q5-1.jpg}
    \caption{
        Plot of dataset X
    }\label{fig:q5-1}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/images/q5-2.jpg}
    \caption{
        Architecture of our simple neural network
    }\label{fig:q5-2}
\end{figure}
Denote the two features \( x_{1} \) and \( x_{2} \) and the three neurons in the hidden layer \( h_{1}, h_{2} \), and \( h_{3} \), and the output of the neuron as o.
Let the weight from \( x_{i} \) to \( h_{j} \) be \( w_{i, j}{ }^{[1]} \) for \( i \in \{1,2\}, j \in \{1,2,3\} \), and the weight from \( h_{j} \) to o be \( A_{j}{ }^{[2]} \).
Finally, denote the intercept weight(biases) for \( h_{j} \) as \( w_{0, j}{ }^{[1]} \), and the intercept(bias) for o as \( w_{0}{ }^{[2]} \).
For the loss function, we'll use average squared loss instead of the usual negative log-likelihood:
\[
    l=\frac{1}{m} \sum_{i=1}^{m}\left(o^{(i)}-y^{(i)}\right)^{2}
\]
where \( o^{(i)} \) is the result of the output neuron for example i.
\begin{enumerate}[label= (\alph*), noitemsep, topsep=0pt]
    \item Suppose we use the sigmoid function as the activation function for \( h_{1}, h_{2}, h_{3} \), and \( o \).
          What is the gradient descent update to \( w_{1,2}{ }^{[1]} \), assuming we use a learning rate of \( \alpha \)?
          Your answer should be written in terms of \( x^{(i)}, o^{(i)}, y^{(i)} \), and the weights.

    \item Now, suppose instead of using the sigmoid function for the activation function of \( h_{1}, h_{2}, h_{3} \), and \( o \), we instead use the step function \( f(x) \), defined as
          \[
              f(x)= \begin{cases}1, & x \geq 0 \\ 0, & x<0\end{cases}
          \]
          Is it possible to have a set of weights that allow the neural network to classify this dataset with \( 100 \% \) accuracy?

    \item Let the activation functions for \( h_{1}, h_{2} \), and \( h_{3} \) be the linear function \( f(x)=x \) and the activation function for \( o \) be the same step function as before.
          Is it possible to have a set of weights that allow the neural network to classify this dataset with \( 100 \% \) accuracy?
\end{enumerate}

\subsection*{Solution}
