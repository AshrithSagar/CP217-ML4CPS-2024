{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP 217: ML4CPS Workshop Week 7\n",
    "\n",
    "## Week 7 Worksheet:   Perceptron from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week, we will implement perceptron model from scratch. We again start with generating training data. As you saw in the lectures a perceptron is a linear classifier. Therefore, we will aim to generate linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_s_shaped_data(gap=3):\n",
    "    x = np.random.randn(80, 2)\n",
    "\n",
    "    x[10:20] += np.array([3, 4]) ## keep this datapoints centered at mean (3,4) for (x,y)\n",
    "    x[20:30] += np.array([0, 8])\n",
    "    x[30:40] += np.array([3, 12])\n",
    "\n",
    "    x[40:50] += np.array([gap, 0])\n",
    "    x[50:60] += np.array([3 + gap, 4])\n",
    "    x[60:70] += np.array([gap, 8])\n",
    "    x[70:80] += np.array([3 + gap, 12])\n",
    "\n",
    "    y = np.hstack([-np.ones(40), np.ones(40)])\n",
    "    \n",
    "    d = collections.namedtuple('Dataset', ['x', 'y'])\n",
    "    d.x = x\n",
    "    d.y = y\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = generate_s_shaped_data(8)\n",
    "x = d.x\n",
    "y = d.y\n",
    "\n",
    "plt.plot(x[y==-1,0], x[y==-1,1], \"o\")\n",
    "plt.plot(x[y==1,0], x[y==1,1], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train a binary classifier on this data. For this weâ€™ll use the perceptron algorithm, which\n",
    "you should recall takes a model of the form\n",
    "$$\\begin{align*}\n",
    " s(\\mathbf{x}) &= w_0 + \\mathbf{w}^T \\mathbf{x} \\\\\n",
    " predict(\\mathbf{x}) &= \\left\\{ \n",
    "\\begin{array}{cc} \n",
    "1, & \\mbox{if $s(\\mathbf{x}) \\geq 0$} \\\\\n",
    "-1, &  \\mbox{otherwise}\n",
    "\\end{array} \\right .\n",
    "\\end{align*}$$\n",
    "\n",
    "Please refer to the lecture notes and the text book for a detailed exposition of the perceptron algorithm and linear classification models in general. Note that we treat $\\mathbf{x}$ as a vector above, here it has two elements, one for each of the input dimensions. How many elements must there be in the weight vector $\\mathbf{w}$?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will use the standard trick to incorporate the bias term $w_0$ into the weights $\\mathbf{w}$ by using a basis function $\\phi(x_1, x_2) = [1~x_1~x_2]^T$ which adds an extra constant dimension. The model becomes\n",
    "$$ s(\\mathbf{x}) = \\mathbf{w}^T \\phi(\\mathbf{x}) $$\n",
    "To do this, simply concatenate a column of 1s to the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.column_stack([np.ones(x.shape[0]), x])\n",
    "Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Phi now has $3$ columns. In this array, each training instance is a row and each column is a feature. From now on we will use Phi instead of x. Each row represents $\\phi(\\mathbf{x})$ for a training instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write the prediction function (aka discriminant). This takes as input a data point (a row from Phi, i.e., a vector of 3 numbers) and the model parameters ($\\mathbf{w}$) and outputs predicted label $1$ or $-1$. Recall that if $s(\\mathbf{x})=0$, the predicted class is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_pred(phi, w):\n",
    "    s= ... # over to you\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to test your prediction function with some examples! Note that it's more useful if it can support phi inputs both as vectors (returning a scalar, either +1/-1) and as matrices (returning a vector of +1/-1 values). The latter allows for you to supply a full dataset in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perc_pred([1, 0, 1], [1, 2, 3]))\n",
    "print(perc_pred(Phi, [1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for training algorithm which fits the weights, $\\mathbf{w}$, to the training data. Recall that this is an online training algorithm, and we are going to iterate through the training examples one by one. Moreover, we are going to do several cycles, called *epochs*, such that we iterate through the entire training set within one epoch. Write a function called *train* which takes the basis data matrix *Phi*, the labels *t* and a number of epochs. This should implement the following pseudo-code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> initialise weights to zero \n",
    "\n",
    "> repeat epoch times\n",
    "\n",
    "> >   for each x and y pair in the training set\n",
    "\n",
    "> > >       if model prediction and y differ, make weight update\n",
    "\n",
    "> return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight update in the inner loop is $\\mathbf{w} \\leftarrow \\mathbf{w} + y\\phi(\\mathbf{x})$.\n",
    "What is the purpose of this update?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the precition function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, target, epochs, w=[], eta = 1):\n",
    "    if len(w) == 0:\n",
    "        w = np.zeros(data.shape[1])\n",
    "    for e in range(epochs):\n",
    "        for i in range(data.shape[0]):\n",
    "            yhat = ... # over to you\n",
    "            if yhat != target[i]:\n",
    "                w = ... # over to you\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your training algorithm for 5 epochs to learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat = train(Phi, y, 5)\n",
    "w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the proportion of misclassified cases as the quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = ... # over to you\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the weights learnt in training. Do these match your intuitions? Plot the decision boundary represented by the weights, $\\mathbf{w}^T \\phi(\\mathbf{x}) = 0$. Solving for $x_2$ as a function of $x_1$ yields $x_2 = -\\frac{w_0}{w_2} - \\frac{w_1}{w_2} x_1$. Note that you can you *linspace* and *plot* for displaying the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-5, 10, 100)\n",
    "x2 = - w_hat[0] / w_hat[2] - w_hat[1] / w_hat[2] * x1\n",
    "\n",
    "# plot the decision boundary \n",
    "plt.plot(x1, x2)\n",
    "plt.ylim(-6, 16)\n",
    "# plot the training data points\n",
    "plt.plot(x[y==-1,0], x[y==-1,1], \"o\")\n",
    "plt.plot(x[y==1,0], x[y==1,1], \"o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Rerun your training with a larger number of epochs (10, 100, 1000), and evaluate how the accuracy changes.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heldout evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating on the training data is not a good idea in general, other than for debugging your algorithms. (Can you explain why?) We are going to generate another synthetic data thus essentially creating a fresh *heldout set*. What is the accuracy on this heldout data, and how does this compare to training accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_held = generate_s_shaped_data(8)\n",
    "x_heldout = d_held.x \n",
    "y_heldout = d_held.y\n",
    "\n",
    "\n",
    "plt.plot(x[y==-1,0], x[y==-1,1], \"o\")\n",
    "plt.plot(x[y==1,0], x[y==1,1], \"o\")\n",
    "\n",
    "# plot the heldout data points\n",
    "plt.plot(x_heldout[y_heldout==-1,0], x_heldout[y_heldout==-1,1], \"x\")\n",
    "plt.plot(x_heldout[y_heldout==1,0], x_heldout[y_heldout==1,1], \"x\")\n",
    "\n",
    "\n",
    "Phi_heldout = np.column_stack([np.ones(x_heldout.shape[0]), x_heldout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = ... # over to you\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decision boundary for heldout data, together with training and heldout datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 6, 100)\n",
    "x2 = - w_hat[0] / w_hat[2] - w_hat[1] / w_hat[2] * x1\n",
    "\n",
    "# plot the decision boundary \n",
    "plt.plot(x1, x2)\n",
    "\n",
    "# plot the training data points\n",
    "plt.plot(x[y==-1,0], x[y==-1,1], \"o\")\n",
    "plt.plot(x[y==1,0], x[y==1,1], \"o\")\n",
    "\n",
    "# plot the heldout data points\n",
    "plt.plot(x_heldout[y_heldout==-1,0], x_heldout[y_heldout==-1,1], \"x\")\n",
    "plt.plot(x_heldout[y_heldout==1,0], x_heldout[y_heldout==1,1], \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the decision boundary separate the points in the two classes? Where do you think the decision boundary should go? And how does the boundary change as you train for longer (more epochs)? Plot train and heldout errors as a function of number epochs. Note that careful tuning of the learning rate is needed to get sensible behaviour. Using $\\eta = \\frac{1}{1+e}$ where $e$ is the epoch number often works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat = []\n",
    "T = 60\n",
    "train_error = np.zeros(T)\n",
    "heldout_error = np.zeros(T)\n",
    "for e in range(T):\n",
    "    # here we use a learning rate, which decays with each epoch\n",
    "    lr = 1./(1+e)\n",
    "    w_hat = ... # over to you\n",
    "    train_error[e] = ... # over to you\n",
    "    heldout_error[e] = ... # over to you\n",
    "\n",
    "plt.plot(train_error, label = 'Train Error')\n",
    "plt.plot(heldout_error, label = 'Held-out Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the heldout error track the training error closely? Is the model (i.e., weights at a given epoch) on the training set the same as the best model on the heldout set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the decision boundary using w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(2, 10, 100)\n",
    "print(w_hat)\n",
    "x2 = - (w_hat[0] / w_hat[2]) - ((w_hat[1] / w_hat[2]) * x1)\n",
    "\n",
    "# plot the training data points\n",
    "plt.plot(x[y==-1,0], x[y==-1,1], \"o\")\n",
    "plt.plot(x[y==1,0], x[y==1,1], \"o\")\n",
    "\n",
    "# plot the heldout data points\n",
    "plt.plot(x_heldout[y_heldout==-1,0], x_heldout[y_heldout==-1,1], \"x\")\n",
    "plt.plot(x_heldout[y_heldout==1,0], x_heldout[y_heldout==1,1], \"x\")\n",
    "\n",
    "# plot the decision boundary \n",
    "plt.plot(x1, x2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate training and heldout datasets that are not linearly separable, and investigate what happens to training and heldout errors with increasing number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of SONAR Signals \n",
    "The task is to train a network to discriminate between sonar signals bounced off a metal cylinder(M)  and those bounced off a roughly cylindrical rock (R).\n",
    "\n",
    "The dataset contains 111 patterns obtained by bouncing sonar signals off a metal cylinder (M) and 97 patterns obtained from rocks (R) at various angles and under various conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. \n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time.\n",
    "\n",
    "The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder).\n",
    "\n",
    "You can learn more about this dataset on the UCI Machine Learning repository:\n",
    "- https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "dataframe = pd.read_csv(\"sonar_all_data.csv\" , header=None)\n",
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0:60].astype(float)\n",
    "y = dataset[:,60]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y[encoded_y==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi=np.column_stack([np.ones(X.shape[0]), X])\n",
    "Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets using Stratified Sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "Phi_train, Phi_test, y_train, y_test = train_test_split(Phi, encoded_y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat = np.zeros(Phi.shape[1])\n",
    "T = 100\n",
    "train_error = np.zeros(T)\n",
    "heldout_error = np.zeros(T)\n",
    "for e in range(T):\n",
    "    lr= 1./(1+e)\n",
    "    w_hat = train(Phi_train, y_train, 1, w_hat, eta= lr)\n",
    "    train_error[e] = np.sum(perc_pred(Phi_train, w_hat) != y_train) / float(y_train.shape[0])\n",
    "    heldout_error[e] = np.sum(perc_pred(Phi_test, w_hat) != y_test) / float(y_test.shape[0])\n",
    "\n",
    "\n",
    "plt.plot(train_error, label = 'Train Error')\n",
    "plt.plot(heldout_error, label = 'Held-out Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = np.sum(perc_pred(Phi_test, w_hat) == y_test) / float(y_test.shape[0])\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Pattern Recognition and Machine Learning, Christopher Bishop, New York, Springer,  2006.\n",
    "2. https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "3. Statistical Machine Learning Course Workshop, 2015, University of Melbourne, Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
